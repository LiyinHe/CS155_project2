{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook implements method 2 for matrix factorization (using stochastic gradient descent with bias terms for each user and movie) and enables visualization of the resulting latent factors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "font_kwargs = {'family': 'sans-serif',\n",
    "               'sans-serif': 'Arial',\n",
    "               'size': 12}\n",
    "plt.rc('font', **font_kwargs)\n",
    "mathtext_kwargs = {'fontset': 'custom',\n",
    "                   'bf': 'Arial:bold',\n",
    "                   'cal': 'Arial:italic',\n",
    "                   'it': 'Arial:italic',\n",
    "                   'rm': 'Arial'}\n",
    "plt.rc('mathtext', **mathtext_kwargs)\n",
    "savefig_kwargs = {'dpi': 300, 'bbox_inches': 'tight',\n",
    "                  'transparent': True}\n",
    "plt.rc('pdf', fonttype=42)\n",
    "#%config InlineBackend.figure_format = 'pdf'\n",
    "%config InlineBackend.print_figure_kwargs = savefig_kwargs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, we define functions to train a model with <i>k</i> latent factors, identifying <i>k</i>&times;<i>m</i> matrix U and <i>k</i>&times;<i>n</i> matrix V representing the <i>m</i> users and <i>n</i> movies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "def grad(Y_ij, U_i, V_j, a_i, b_j, mu=0, reg=0):\n",
    "    '''\n",
    "    Takes as input training point Y_ij (rating of ith user for\n",
    "    jth movie), user vector U_i (ith column of U), movie vector\n",
    "    V_j (jth column of V), user deviation a_i, and movie deviation\n",
    "    b_j, with optional arguments for global bias mu (assumed to be\n",
    "    0 by default) and regularization strength reg (set to 0 by\n",
    "    default).\n",
    "    \n",
    "    Returns the gradients of the regularized squared loss function\n",
    "    with respect to U_i, V_j, a_i, and b_j.\n",
    "    '''\n",
    "    grad_U = reg * U_i - (Y_ij - mu - np.dot(U_i, V_j) - a_i - b_j) * V_j\n",
    "    grad_V = reg * V_j - (Y_ij - mu - np.dot(U_i, V_j) - a_i - b_j) * U_i\n",
    "    grad_a = reg * a_i - (Y_ij - mu - np.dot(U_i, V_j) - a_i - b_j)\n",
    "    grad_b = reg * b_j - (Y_ij - mu - np.dot(U_i, V_j) - a_i - b_j)\n",
    "    \n",
    "    return grad_U, grad_V, grad_a, grad_b\n",
    "\n",
    "def err(Y, U, V, a, b, mu=None, reg=0):\n",
    "    '''\n",
    "    Takes as input Y, a matrix where each row (i, j, Y_ij) gives the\n",
    "    rating Y_ij of user i for movie j; U, a matrix of shape (k, m)\n",
    "    where each column represents one of the m users; V, a matrix of\n",
    "    shape (k, n) where each column represents one of the n movies;\n",
    "    a, a vector of shape (m, ) where each value represents the\n",
    "    user-specific deviation from the global mean; b, a vector of shape\n",
    "    (n, ) where each value represents the movie-specific deviation from\n",
    "    the global mean; mu, a scalar giving the global bias for ratings\n",
    "    (computed from Y if set to None); and reg, a scalar giving the\n",
    "    regularization strength (set to 0 by default).\n",
    "    \n",
    "    Returns the mean regularized squared error for predictions made\n",
    "    by estimating Y_ij as the dot product of the ith column of U and\n",
    "    the jth column of V (accounting for global, user, and movie biases).\n",
    "    '''\n",
    "    # Compute term for regularization\n",
    "    err_r = reg * (np.linalg.norm(U) + np.linalg.norm(V) +\n",
    "                   np.linalg.norm(a) + np.linalg.norm(b)) / 2\n",
    "    \n",
    "    # Compute term for squared loss\n",
    "    if mu is None:\n",
    "        mu = np.mean(Y_ij[:, -1])\n",
    "    resid = np.array([Y_ij - mu - np.dot(U[:, i], V[:, j]) - a[i] - b[j]\n",
    "                      for (i, j, Y_ij) in Y])\n",
    "    err_s = np.sum(resid ** 2) / 2\n",
    "    \n",
    "    # Return mean regularized squared error\n",
    "    return (err_r + err_s) / len(Y)\n",
    "\n",
    "def process_Y(Y_in):\n",
    "    '''\n",
    "    Takes as input Y_in, a matrix where each row (i, j, Y_ij) gives the\n",
    "    rating Y_ij of user i for movie j.\n",
    "    \n",
    "    Returns Y, a matrix of the same size as Y_in where user and movie IDs\n",
    "    have been mapped to sorted indices (lowest value has index 0, etc.).\n",
    "    '''\n",
    "    Y = np.zeros(Y_in.shape)\n",
    "    \n",
    "    # Map user and movie IDs to indices\n",
    "    ids_U = np.unique(Y_in[:, 0]).astype(int)\n",
    "    inds_U = -np.ones(np.max(ids_U) + 1)\n",
    "    for i, id_U in enumerate(ids_U):\n",
    "        inds_U[id_U] = i\n",
    "    ids_V = np.unique(Y_in[:, 1]).astype(int)\n",
    "    inds_V = -np.ones(np.max(ids_V).astype(int) + 1)\n",
    "    for i, id_V in enumerate(ids_V):\n",
    "        inds_V[id_V] = i\n",
    "    \n",
    "    # Convert IDs in Y_in to indices\n",
    "    for r, (i, j, Y_ij) in enumerate(Y_in):\n",
    "        Y[r] = [inds_U[i], inds_V[j], Y_ij]\n",
    "    Y = Y.astype(int)\n",
    "    \n",
    "    return Y\n",
    "    \n",
    "def train_model(Y_in, k, m=None, n=None, reg=0, eta=0.01,\n",
    "                eps=0.0001, max_epochs=300):\n",
    "    '''\n",
    "    Takes as input Y_in, a matrix where each row (i, j, Y_ij) gives the\n",
    "    rating Y_ij of user i for movie j; k, the number of latent factors;\n",
    "    m, the number of users (inferred from Y if set to None); n, the\n",
    "    number of movies (inferred from Y if set to None); reg, the strength\n",
    "    of regularization (set to 0 by default); eta, the learning rate\n",
    "    (set to 0.01 by default); eps, the minimum improvement in mean\n",
    "    regularized squared error at each epoch as a proportion of the\n",
    "    improvement after the first epoch (set to 0.0001 by default); and\n",
    "    max_epochs, the maximum number of epochs (set to 300 by default).\n",
    "    \n",
    "    Returns trained latent factor model given by matrices U and V and\n",
    "    bias vectors a and b, along with the unregularized mean squared error\n",
    "    of the model.\n",
    "    '''\n",
    "    Y = process_Y(Y_in)\n",
    "    \n",
    "    # Extract numbers of users and movies if needed\n",
    "    if m is None:\n",
    "        m = len(np.unique(Y[:, 0]))\n",
    "    if n is None:\n",
    "        n = len(np.unique(Y[:, 1]))\n",
    "    \n",
    "    # Initialize entries of U, V, a, and b to random numbers in [-0.5, 0.5]\n",
    "    U = np.random.random((k, m)) - 0.5\n",
    "    V = np.random.random((k, n)) - 0.5\n",
    "    a = np.random.random(m) - 0.5\n",
    "    b = np.random.random(n) - 0.5\n",
    "    mu = np.mean(Y[:, -1])\n",
    "    \n",
    "    # Store loss (regularized mean squared error) for each epoch\n",
    "    loss = np.empty(max_epochs + 1)\n",
    "    loss[0] = err(Y, U, V, a, b, mu, reg)\n",
    "    \n",
    "    # Perform SGD for specified number of epochs (or until termination)\n",
    "    for epoch in range(max_epochs):\n",
    "        # Shuffle data\n",
    "        np.random.shuffle(Y)\n",
    "        \n",
    "        # Update U, V, a, and b for each data point\n",
    "        for (i, j, Y_ij) in Y:\n",
    "            grad_U, grad_V, grad_a, grad_b = \\\n",
    "                grad(Y_ij, U[:, i], V[:, j], a[i], b[j], mu=mu, reg=reg)\n",
    "            U[:, i] -= eta * grad_U\n",
    "            V[:, j] -= eta * grad_V\n",
    "            a[i] -= eta * grad_a\n",
    "            b[j] -= eta * grad_b\n",
    "        \n",
    "        # Compute loss and check for stopping condition\n",
    "        loss[epoch + 1] = err(Y, U, V, a, b, mu, reg)\n",
    "        if (loss[epoch + 1] - loss[epoch]) <= eps * (loss[1] - loss[0]):\n",
    "            break\n",
    "    \n",
    "    # Compute unregularized mean squared error for final model\n",
    "    err_model = err(Y, U, V, a, b, mu, reg=0)\n",
    "    return U, V, a, b, err_model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can then train our model for the MovieLens dataset.  We start by reading in the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read in data on ratings\n",
    "Y_train = pd.read_csv(os.path.join('data', 'train.txt'),\n",
    "                      sep='\\t', header=None,\n",
    "                      names=['User ID', 'Movie ID', 'Rating'])\n",
    "Y_test = pd.read_csv(os.path.join('data', 'test.txt'),\n",
    "                     sep='\\t', header=None,\n",
    "                     names=['User ID', 'Movie ID', 'Rating'])\n",
    "\n",
    "# Read in information on movies\n",
    "names = ['Movie ID', 'Movie Title', 'Unknown', 'Action',\n",
    "         'Adventure', 'Animation', 'Children\\'s', 'Comedy',\n",
    "         'Crime', 'Documentary', 'Drama', 'Fantasy',\n",
    "         'Film-Noir', 'Horror', 'Musical', 'Mystery',\n",
    "         'Romance', 'Sci-Fi', 'Thriller', 'War', 'Western']\n",
    "movies = pd.read_csv(os.path.join('data', 'movies.txt'),\n",
    "                     sep='\\t', header=None,\n",
    "                     encoding='latin_1', names=names)\n",
    "movies['Movie Title'] = movies['Movie Title'].str.strip()\n",
    "\n",
    "# Separate movie name and year if desired\n",
    "split_year = False\n",
    "if split_year:\n",
    "    movies.loc[266, 'Movie Title'] = 'unknown (0000)'\n",
    "    movies.loc[1411, 'Movie Title'] = \\\n",
    "        'Land Before Time III: The Time of the Great Giving (V) (1995)'\n",
    "    movies['Year'] = [int(title[-5:-1]) for title in movies['Movie Title']]\n",
    "    movies['Movie Title'] = [title[:-7] for title in movies['Movie Title']]\n",
    "\n",
    "# Merge ratings data with movie metadata\n",
    "data_train = Y_train.merge(movies, how='left', on='Movie ID')\n",
    "data_test = Y_test.merge(movies, how='left', on='Movie ID')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We then perform hyperparameter selection for regularization strength and learning rate using a subset of the training data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Regularization 0 and learning rate 0.0001\n",
      "Regularization 0 and learning rate 0.001\n",
      "Regularization 0 and learning rate 0.01\n",
      "Regularization 0 and learning rate 0.1\n",
      "Regularization 0.0001 and learning rate 0.0001\n",
      "Regularization 0.0001 and learning rate 0.001\n",
      "Regularization 0.0001 and learning rate 0.01\n",
      "Regularization 0.0001 and learning rate 0.1\n",
      "Regularization 0.001 and learning rate 0.0001\n",
      "Regularization 0.001 and learning rate 0.001\n",
      "Regularization 0.001 and learning rate 0.01\n",
      "Regularization 0.001 and learning rate 0.1\n",
      "Regularization 0.01 and learning rate 0.0001\n",
      "Regularization 0.01 and learning rate 0.001\n",
      "Regularization 0.01 and learning rate 0.01\n",
      "Regularization 0.01 and learning rate 0.1\n",
      "Regularization 0.1 and learning rate 0.0001\n",
      "Regularization 0.1 and learning rate 0.001\n",
      "Regularization 0.1 and learning rate 0.01\n",
      "Regularization 0.1 and learning rate 0.1\n",
      "Regularization 1 and learning rate 0.0001\n",
      "Regularization 1 and learning rate 0.001\n",
      "Regularization 1 and learning rate 0.01\n",
      "Regularization 1 and learning rate 0.1\n"
     ]
    }
   ],
   "source": [
    "# Specify regularization strengths and learning rates to test\n",
    "regs = [0, 0.0001, 0.001, 0.01, 0.1, 1]\n",
    "etas = [0.0001, 0.001, 0.01, 0.1]\n",
    "\n",
    "# Specify number of latent factors\n",
    "k = 20\n",
    "\n",
    "# Extract subset of the training data\n",
    "Y_sub = Y_train.values[np.random.choice(len(Y_train), 20000, replace=False)]\n",
    "Y_sub = process_Y(Y_sub)\n",
    "Y_sub_train = Y_sub[:18000]\n",
    "Y_sub_test = Y_sub[18000:]\n",
    "\n",
    "# Save training and testing errors\n",
    "err_train = np.zeros((len(regs), len(etas)))\n",
    "err_test = np.zeros((len(regs), len(etas)))\n",
    "\n",
    "# Train model for each set of hyperparameters\n",
    "mu = np.mean(Y_sub_train[:, -1])\n",
    "for i, reg in enumerate(regs):\n",
    "    for j, eta in enumerate(etas):\n",
    "        print('Regularization {} and learning rate {}'.format(reg, eta))\n",
    "        U, V, a, b, err_model = train_model(Y_sub_train, k=k,\n",
    "                                            m = len(np.unique(Y_sub[:, 0])),\n",
    "                                            n = len(np.unique(Y_sub[:, 1])),\n",
    "                                            reg=reg, eta=eta,\n",
    "                                            eps=0.0001, max_epochs=300)\n",
    "        err_train[i, j] = err_model\n",
    "        err_test[i, j] = err(Y_sub_test, U, V, a, b, mu=mu, reg=reg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y_sub_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.78192849, 0.77260038, 0.60141646, 0.27591038],\n",
       "       [0.78757331, 0.75623376, 0.59729168, 0.27851863],\n",
       "       [0.77741859, 0.76951895, 0.59436938, 0.28177306],\n",
       "       [0.7719383 , 0.75038532, 0.59620854, 0.28945989],\n",
       "       [0.78444286, 0.75088619, 0.58907561, 0.32420093],\n",
       "       [0.81259424, 0.74731995, 0.56830247, 0.46332773]])"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "err_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.78449764, 0.78337127, 0.67221776, 0.55908496],\n",
       "       [0.81155827, 0.78640541, 0.66453664, 0.55821759],\n",
       "       [0.79041615, 0.80670676, 0.66921852, 0.57985224],\n",
       "       [0.81319759, 0.7775822 , 0.6727094 , 0.5922801 ],\n",
       "       [0.83163458, 0.78059501, 0.67244226, 0.54453889],\n",
       "       [0.82892163, 0.80731727, 0.64943603, 0.55102436]])"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "err_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main():\n",
    "    print(\"Factorizing with \", M, \" users, \", N, \" movies.\")\n",
    "    Ks = [10,20,30,50,100]\n",
    "\t\n",
    "    reg = 0.0\n",
    "    eta = 0.03 # learning rate\n",
    "    E_in = []\n",
    "    E_out = []\n",
    "\t\n",
    "    # Use to compute Ein and Eout\n",
    "    for K in Ks:\n",
    "        U,V, err = train_model(M, N, K, eta, reg, Y_train)\n",
    "        E_in.append(err)\n",
    "        E_out.append(get_err(U, V, Y_test))\n",
    "\t\n",
    "    plt.plot(Ks, E_in, label='$E_{in}$')\n",
    "    plt.plot(Ks, E_out, label='$E_{out}$')\n",
    "    plt.title('Error vs. K')\n",
    "    plt.xlabel('K')\n",
    "    plt.ylabel('Error')\n",
    "    plt.legend()\n",
    "    plt.savefig('2D.png')\n",
    "\n",
    "   \n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
